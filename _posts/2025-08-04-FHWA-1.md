---
title: Benchmarking I: A first look at the FHWA Asphalt Framework 
date: 2025-08-04 
categories: [Demos]
tags: [solutions, benchmarking, fhwa, fedcommons, background]     # TAG names should always be lowercase
---
Antelope is a complete (though not yet fully comprehensive) LCA computing environment, but the first task it was created for is benchmarking of existing datasets. Today we are going to look at a Federal LCA Commons dataset that is the product of years of multi-stakeholder work across industry, academia, and state and federal government agencies: the Hot Mix Asphalt LCA Framework model prepared under the Federal Highway Administration's (FHWA) Sustainable Pavement Project.  Read all about the Asphalt Framework [here](https://rosap.ntl.bts.gov/view/dot/38470) (if you prefer 15-MB government project reports) or [here](https://link.springer.com/article/10.1007/s11367-020-01777-x) if you prefer paywalled academic articles.

You can follow along with this article with the [walkthrough]() (jupyter) in the User Support repository.

# Overview of the data source

The Asphalt Framework is a guidance document describing how LCA could be used effectively for pavement procurement.  The study, on the other hand, is an actual LCA that was conducted in support of a PCR for hot mix asphalt on behalf of the National Asphalt and Pavement Association (NAPA).  Read the [LCA study report](https://www.asphaltpavement.org/uploads/documents/EPD_Program/NAPA_LCA_2016.pdf).


## Obtaining the Repository

The study model was submitted to the US Federal LCA Commons. So, we go there to download it.

[![Click "Download"](/assets/img/posts/FHWA-1-commons.png)](https://www.lcacommons.gov/lca-collaboration/Federal_Highway_Administration/mtu_pavement/datasets)

We download the full dataset as JSON-LD 2.  Save it on a computer in a location you remember.

## Accessing the Repository

We do our work in a catalog.  The catalog doesn't depend on any resources on our computer to start, other than the default flow content that comes bundled with `antelope_core`.

```python
from antelope_core import LcCatalog
cat = LcCatalog()
```

We add a new resource that connects the study and assigns it an origin.  A resource has four required components: an **origin** which you use to refer to it; a **source** which points to the data, a **data source type** which tells Antelope what kind of data provider to use to access the data, and a list of **interfaces** the data supports.

In Antelope, resources provide different types of information about the LCA data via different [interfaces](https://antelopelca.github.io/antelope/interfaces.html). Here's what each interface provides:
 - `basic`: documentary information (properties)
 - `exchange`: data about process inventories and exchange values
 - `quantity`: data about flow properties and characterization.

Generally OpenLCA datasets include all three.

> [!TIP]
> Idea! Maybe different data source types should have sensible default interface values.



```python
cat.new_resource('my.fhwa`, `/path/to/Federal_Highway_Administration-mtu_pavement.zip', 'OpenLcaJsonLdArchive',
                 ('basic', 'exchange', 'quantity'))
cat.show_interfaces()
'''
my.fhwa [basic, exchange, quantity]
local.qdb [basic, index, quantity]
'''
```

Then we can interact with the resource by querying it.  

```python
q = cat.query('my.fhwa')
```
At this point we can retrieve specific datasets if we know their IDs, but we can't do search or discovery because the data source is not yet indexed.  So, we can search on the Federal LCA commons and identify datasets to retrieve:

```python
p = q.get('72d5a381-8cae-4e1d-b0a3-26cc43b69867')
p.show()
'''
ProcessRef catalog reference (72d5a381-8cae-4e1d-b0a3-26cc43b69867)
origin: my.fhwa
   UUID: 72d5a381-8cae-4e1d-b0a3-26cc43b69867
   Name: Asphalt binder, 8% ground rubber tire (GRT), consumption mix, at terminal, from crude oil, 8% ground rubber tire
Comment: 
Exchange ID 629 (e10e86a1-fd6a-3ac5-a822-55762e8ae99d): Conversion Error from unit kBq to kg
Exchange ID 1053 (bdff254b-ff47-305e-99ee-ff2dddd46876): Conversion Error from unit kBq to kg
==Local Fields==
        Classifications: ['42: Wholesale Trade', '4247: Petroleum and Petroleum Products Merchant Wholesalers']
           SpatialScope: Northern America
          TemporalScope: {'begin': '2015-12-31-05:00', 'end': '2022-12-31-05:00'}
                  @type: Process
            description: This cradle-to-gate dataset covers all relevant process steps and technologies for production of asphalt binder with high overall data quality. The inventory is based on primary data from twelve refineries and eleven terminals in North America. The product boundary....
'''
```

However, the data source by itself is not sufficient to perform LCI or LCIA operations because those require a linked technology matrix. In Antelope, this is delivered via the **background** interface, which depends on an **index** of the data.

## Adding supporting interfaces

First, we index the data source- this means loading all its processes.

```python
cat.index_ref('my.fhwa')
"""
Loading /data/LCI/FedCommons/Federal_Highway_Administration-mtu_pavement.zip
fedcommons.fhwa.index.20250805: None
fedcommons.fhwa.index.20250805: Setting NSUUID (None) 6f9c793c-a94a-4b1b-b7c4-39de328ef486
Ignoring ns_uuid specification
fedcommons.fhwa: /data/LCI/FedCommons/Federal_Highway_Administration-mtu_pavement.zip
'my.fhwa.index.20250805'
"""
```

Now we can see that an index interface was added to the catalog:

```python
cat.show_interfaces()
"""
my.fhwa [basic, exchange, quantity]
my.fhwa.index.20250805 [basic, index]
local.qdb [basic, index, quantity]
"""
```
Now we can do things like counting and searching. We can also investigate linking the database for LCI computation.

## Linking for LCI
The Antelope [Background engine](https://github.com/AntelopeLCA/background) performs Tarjan ordering of Exchange data to detect strongly-connected components (i.e. collections of processes where everything depends on everything). Building this network relies on every *dependent* exchange being linked to a reference exchange of another process. The linking algorithm has a few ways to determine an appropriate exchange.

1. In most data sources, an exchange can have a "Preferred provider" (OpenLCA) or "ActivityLink" made explicit. These are always followed.
2. Some flows may have only one viable *target*, i.e. there is precisely one database process that provides the given flow as a reference. 
3. Some flows may have *no* viable targets, in which case they become cutoffs (like emissions, but into the modeling environment instead of the natural environment)
4. When flows have more than one viable target, the user must specify a preferred provider for each ambiguous flow, or else specify an algorithmic approach to pick one. At present, the only algorithmic choices available are "first" and "last" (alphabetically), "cutoff", or "abort" (the default).

We can learn a lot about a dataset by inspecting its linking characteristics. There's a tool for this called `CheckTerms`. The tool requires a query with the index and exchange interfaces, and inspects each exchange for valid terminations.  For our FHWA database it tells us the following: 

```python
from antelope_core import CheckTerms
check = CheckTerms(q)
"""
1298 processes
1485 reference exchanges
260476 dependent exchanges
anchored: 6952 exchanges
cutoff: 4832 exchanges
elementary: 248663 exchanges
self: 16 exchanges

missing: 11 exchanges
ambiguous: 2 exchanges
"""
```
